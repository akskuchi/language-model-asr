@article{Bengio94,
  author = {Yoshua Bengio, Patrice Simard and Paolo Frasconi},
  title = {Learning Long-Term Dependencies with Gradient Descent is Difficult},
  journal = {IEEE Transactions on Neural Networks},
  volume = {5},
  number = {10},
  pages = {157 - 166},
  year = {1994},
  url = {http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf}
}

@article{hochreiter97,
  author = {Sepp Hochreiter, Jurgen Schmidhuber},
  title = {Long Short-Term Memory},
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735 - 1780},
  year = {1997},
  url = {http://www.bioinf.jku.at/publications/older/2604.pdf}
}

@InProceedings{schwenk05,
  author = {Holger Schwenk and Jean-Luc Gauvain},
  title = {Training Neural Network Language Models on Very Large Corpora},
  booktitle = {Joint Conference HLT/EMNLP},
  pages = {201 - 208},
  year = {2005},
  url = {ftp://tlp.limsi.fr/public/emnlp05.pdf},
  note = {Accessed 6 Nov. 2018}
}
  
@InProceedings{Milkolov10,
  author = {Tomas Milkolov, Martin Karafiat, Lukas Burget, Jan Cernocky, Sanjeev Khudanpur},
  title = {Recurrent neural network based language model},
  booktitle = {INTERSPEECH 2010},
  pages = {1045 - 1048},
  url = {https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf},
  year = {2010}
}
  
@InProceedings{xavier10,
  author = {Xavier Glorot, Yoshua Bengio},
  title = {Understanding the difficulty of training deep feedforward neural networks},
  booktitle = {Proceedings of the13thInternational Conferenceon Artificial Intelligence and Statistics (AISTATS) 2010},
  pages = {249 - 256},
  url = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  year = {2010}
}

@misc{hinton2012,
  author = {G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever and R. R. Salakhutdinov},
  title = {Improving neural networks by preventing co-adaptation of feature detectors},
  url = {https://arxiv.org/pdf/1207.0580.pdf},
  year = {2012}
}

@misc{cho2014,
  author = {Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio},
  title = {Learning Phrase Representations using RNN Encoderâ€“Decoderfor Statistical Machine Translation},
  url = {https://arxiv.org/pdf/1406.1078v3.pdf},
  year = {2014}
}
  
@misc{colah2015,
  author = {Chris Olah},
  title = {Understanding LSTM Networks},
  url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
  year = {2015}
}

@InProceedings{press16,
  title =    {Understanding the difficulty of training deep feedforward neural networks},
  author =   {Xavier Glorot and Yoshua Bengio},
  booktitle =    {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages =    {249--256},
  year =   {2010},
  editor =   {Yee Whye Teh and Mike Titterington},
  volume =   {9},
  series =   {Proceedings of Machine Learning Research},
  address =    {Chia Laguna Resort, Sardinia, Italy},
  month =    {13--15 May},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url =    {http://proceedings.mlr.press/v9/glorot10a.html},
  abstract =   {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@misc{zaremba2015,
  title = {Recurrent Neural Network Regularization},
  author = {Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals},
  url = {https://arxiv.org/pdf/1409.2329.pdf},
}

@misc{ioffe,
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = {Ioffe S. Szegedy C.},
  url = {https://arxiv.org/pdf/1502.03167.pdf},
  year = {2015},
  note = {Accessed 7 Nov. 2018}
}

@misc{courville2017,
  title = {Recurrent Batch Normalization},
  author = {Tim Cooijmans, Nicolas Ballas, Cesar Laurent, Caglar Gulcehre and Aaron Courville},
  url = {https://arxiv.org/pdf/1603.09025.pdf},
  year = {2017}
}

@article{bengio08,
  author = {Yoshua Bengio, and J.S Senecal},
  title = {Adaptive Importance Sampling to Accelerate Training of  a Neural Probabilistic  Language Model},
  journal = {IEEE Transactions on Neural Networks},
  url = {http://www.umontreal.ca/~lisa/publications2/index.php/attachments/single/177},
  note = {Accessed 6 Nov. 2018},
  year = {2008}
}

@misc{ling2016,
  author = {Wang Ling, Tiago Luis, Luis Marujo, Ramon Fernandex Astudillo, Silvio Amir, Chris Dyer, Alan W Beck, Isabel Transcoso},
  title = {Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation},
  url = {https://arxiv.org/pdf/1508.02096.pdf},
  year = {2016}
}

@misc{luong2016,
  author = {Minh-Thang Luong and Christopher D. Manning},
  title = {Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models},
  url = {https://arxiv.org/pdf/1604.00788.pdf},
  year = {2016}
}

@misc{pytorchlm,
  author = {The PyTorch Authors},
  title = {Word-level language modeling RNN},
  url = {https://github.com/pytorch/examples/blob/master/word_language_model/main.py},
  year = {2018}
}

@misc{nlgeval,
  author = {Shikhar Sharma, Layla El Asri, Hannes Schulz and Jermie Zumer},
  title = {Relevance of Unsupervised Metrics in Task Oriented Dialogue for Evaluating Natural Language Generation},
  year = {2017},
  url = {https://arxiv.org/pdf/1706.09799.pdf}
 }